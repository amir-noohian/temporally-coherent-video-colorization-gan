{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fid_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU2wAFd5-azW",
        "outputId": "965abb81-a37c-4808-f6a2-2f985dd8d8e5"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CInU2_rH8cGB"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torchvision.models import inception_v3\r\n",
        "import cv2\r\n",
        "import multiprocessing\r\n",
        "import numpy as np\r\n",
        "import glob\r\n",
        "import os\r\n",
        "from scipy import linalg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHcWaGyY8qeM"
      },
      "source": [
        "def to_cuda(elements):\r\n",
        "    \"\"\"\r\n",
        "    Transfers elements to cuda if GPU is available\r\n",
        "    Args:\r\n",
        "        elements: torch.tensor or torch.nn.module\r\n",
        "        --\r\n",
        "    Returns:\r\n",
        "        elements: same as input on GPU memory, if available\r\n",
        "    \"\"\"\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        return elements.cuda()\r\n",
        "    return elements"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSCRNuo68tLI"
      },
      "source": [
        "class PartialInceptionNetwork(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, transform_input=True):\r\n",
        "        super().__init__()\r\n",
        "        self.inception_network = inception_v3(pretrained=True)\r\n",
        "        self.inception_network.Mixed_7c.register_forward_hook(self.output_hook)\r\n",
        "        self.transform_input = transform_input\r\n",
        "\r\n",
        "    def output_hook(self, module, input, output):\r\n",
        "        # N x 2048 x 8 x 8\r\n",
        "        self.mixed_7c_output = output\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            x: shape (N, 3, 299, 299) dtype: torch.float32 in range 0-1\r\n",
        "        Returns:\r\n",
        "            inception activations: torch.tensor, shape: (N, 2048), dtype: torch.float32\r\n",
        "        \"\"\"\r\n",
        "        assert x.shape[1:] == (3, 299, 299), \"Expected input shape to be: (N,3,299,299)\" +\\\r\n",
        "                                             \", but got {}\".format(x.shape)\r\n",
        "        x = x * 2 -1 # Normalize to [-1, 1]\r\n",
        "\r\n",
        "        # Trigger output hook\r\n",
        "        self.inception_network(x)\r\n",
        "\r\n",
        "        # Output: N x 2048 x 1 x 1 \r\n",
        "        activations = self.mixed_7c_output\r\n",
        "        activations = torch.nn.functional.adaptive_avg_pool2d(activations, (1,1))\r\n",
        "        activations = activations.view(x.shape[0], 2048)\r\n",
        "        return activations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX5mbtfH8wqx"
      },
      "source": [
        "def get_activations(images, batch_size):\r\n",
        "    \"\"\"\r\n",
        "    Calculates activations for last pool layer for all iamges\r\n",
        "    --\r\n",
        "        Images: torch.array shape: (N, 3, 299, 299), dtype: torch.float32\r\n",
        "        batch size: batch size used for inception network\r\n",
        "    --\r\n",
        "    Returns: np array shape: (N, 2048), dtype: np.float32\r\n",
        "    \"\"\"\r\n",
        "    assert images.shape[1:] == (3, 299, 299), \"Expected input shape to be: (N,3,299,299)\" +\\\r\n",
        "                                              \", but got {}\".format(images.shape)\r\n",
        "\r\n",
        "    num_images = images.shape[0]\r\n",
        "    inception_network = PartialInceptionNetwork()\r\n",
        "    inception_network = to_cuda(inception_network)\r\n",
        "    inception_network.eval()\r\n",
        "    n_batches = int(np.ceil(num_images  / batch_size))\r\n",
        "    inception_activations = np.zeros((num_images, 2048), dtype=np.float32)\r\n",
        "    for batch_idx in range(n_batches):\r\n",
        "        start_idx = batch_size * batch_idx\r\n",
        "        end_idx = batch_size * (batch_idx + 1)\r\n",
        "\r\n",
        "        ims = images[start_idx:end_idx]\r\n",
        "        ims = to_cuda(ims)\r\n",
        "        activations = inception_network(ims)\r\n",
        "        activations = activations.detach().cpu().numpy()\r\n",
        "        assert activations.shape == (ims.shape[0], 2048), \"Expexted output shape to be: {}, but was: {}\".format((ims.shape[0], 2048), activations.shape)\r\n",
        "        inception_activations[start_idx:end_idx, :] = activations\r\n",
        "    return inception_activations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSXcLq45801q"
      },
      "source": [
        "def calculate_activation_statistics(images, batch_size):\r\n",
        "    \"\"\"Calculates the statistics used by FID\r\n",
        "    Args:\r\n",
        "        images: torch.tensor, shape: (N, 3, H, W), dtype: torch.float32 in range 0 - 1\r\n",
        "        batch_size: batch size to use to calculate inception scores\r\n",
        "    Returns:\r\n",
        "        mu:     mean over all activations from the last pool layer of the inception model\r\n",
        "        sigma:  covariance matrix over all activations from the last pool layer \r\n",
        "                of the inception model.\r\n",
        "    \"\"\"\r\n",
        "    act = get_activations(images, batch_size)\r\n",
        "    mu = np.mean(act, axis=0)\r\n",
        "    sigma = np.cov(act, rowvar=False)\r\n",
        "    return mu, sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpGmN7FW832R"
      },
      "source": [
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\r\n",
        "    \"\"\"Numpy implementation of the Frechet Distance.\r\n",
        "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\r\n",
        "    and X_2 ~ N(mu_2, C_2) is\r\n",
        "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\r\n",
        "            \r\n",
        "    Stable version by Dougal J. Sutherland.\r\n",
        "    Params:\r\n",
        "    -- mu1 : Numpy array containing the activations of the pool_3 layer of the\r\n",
        "             inception net ( like returned by the function 'get_predictions')\r\n",
        "             for generated samples.\r\n",
        "    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted\r\n",
        "               on an representive data set.\r\n",
        "    -- sigma1: The covariance matrix over activations of the pool_3 layer for\r\n",
        "               generated samples.\r\n",
        "    -- sigma2: The covariance matrix over activations of the pool_3 layer,\r\n",
        "               precalcualted on an representive data set.\r\n",
        "    Returns:\r\n",
        "    --   : The Frechet Distance.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    mu1 = np.atleast_1d(mu1)\r\n",
        "    mu2 = np.atleast_1d(mu2)\r\n",
        "\r\n",
        "    sigma1 = np.atleast_2d(sigma1)\r\n",
        "    sigma2 = np.atleast_2d(sigma2)\r\n",
        "\r\n",
        "    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\r\n",
        "    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\r\n",
        "\r\n",
        "    diff = mu1 - mu2\r\n",
        "    # product might be almost singular\r\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\r\n",
        "    if not np.isfinite(covmean).all():\r\n",
        "        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\r\n",
        "        warnings.warn(msg)\r\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\r\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\r\n",
        "\r\n",
        "    # numerical error might give slight imaginary component\r\n",
        "    if np.iscomplexobj(covmean):\r\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\r\n",
        "            m = np.max(np.abs(covmean.imag))\r\n",
        "            raise ValueError(\"Imaginary component {}\".format(m))\r\n",
        "        covmean = covmean.real\r\n",
        "\r\n",
        "    tr_covmean = np.trace(covmean)\r\n",
        "\r\n",
        "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVhA31MU9AHe"
      },
      "source": [
        "def preprocess_image(im):\r\n",
        "    \"\"\"Resizes and shifts the dynamic range of image to 0-1\r\n",
        "    Args:\r\n",
        "        im: np.array, shape: (H, W, 3), dtype: float32 between 0-1 or np.uint8\r\n",
        "    Return:\r\n",
        "        im: torch.tensor, shape: (3, 299, 299), dtype: torch.float32 between 0-1\r\n",
        "    \"\"\"\r\n",
        "    assert im.shape[2] == 3\r\n",
        "    assert len(im.shape) == 3\r\n",
        "    if im.dtype == np.uint8:\r\n",
        "        im = im.astype(np.float32) / 255\r\n",
        "    im = cv2.resize(im, (299, 299))\r\n",
        "    im = np.rollaxis(im, axis=2)\r\n",
        "    im = torch.from_numpy(im)\r\n",
        "    assert im.max() <= 1.0\r\n",
        "    assert im.min() >= 0.0\r\n",
        "    assert im.dtype == torch.float32\r\n",
        "    assert im.shape == (3, 299, 299)\r\n",
        "\r\n",
        "    return im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMtluTRM9DUw"
      },
      "source": [
        "def preprocess_images(images, use_multiprocessing):\r\n",
        "    \"\"\"Resizes and shifts the dynamic range of image to 0-1\r\n",
        "    Args:\r\n",
        "        images: np.array, shape: (N, H, W, 3), dtype: float32 between 0-1 or np.uint8\r\n",
        "        use_multiprocessing: If multiprocessing should be used to pre-process the images\r\n",
        "    Return:\r\n",
        "        final_images: torch.tensor, shape: (N, 3, 299, 299), dtype: torch.float32 between 0-1\r\n",
        "    \"\"\"\r\n",
        "    if use_multiprocessing:\r\n",
        "        with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\r\n",
        "            jobs = []\r\n",
        "            for im in images:\r\n",
        "                job = pool.apply_async(preprocess_image, (im,))\r\n",
        "                jobs.append(job)\r\n",
        "            final_images = torch.zeros(images.shape[0], 3, 299, 299)\r\n",
        "            for idx, job in enumerate(jobs):\r\n",
        "                im = job.get()\r\n",
        "                final_images[idx] = im#job.get()\r\n",
        "    else:\r\n",
        "        final_images = torch.stack([preprocess_image(im) for im in images], dim=0)\r\n",
        "    assert final_images.shape == (images.shape[0], 3, 299, 299)\r\n",
        "    assert final_images.max() <= 1.0\r\n",
        "    assert final_images.min() >= 0.0\r\n",
        "    assert final_images.dtype == torch.float32\r\n",
        "    return final_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCw1Z3_W9H4t"
      },
      "source": [
        "def calculate_fid(images1, images2, use_multiprocessing, batch_size):\r\n",
        "    \"\"\" Calculate FID between images1 and images2\r\n",
        "    Args:\r\n",
        "        images1: np.array, shape: (N, H, W, 3), dtype: np.float32 between 0-1 or np.uint8\r\n",
        "        images2: np.array, shape: (N, H, W, 3), dtype: np.float32 between 0-1 or np.uint8\r\n",
        "        use_multiprocessing: If multiprocessing should be used to pre-process the images\r\n",
        "        batch size: batch size used for inception network\r\n",
        "    Returns:\r\n",
        "        FID (scalar)\r\n",
        "    \"\"\"\r\n",
        "    images1 = preprocess_images(images1, use_multiprocessing)\r\n",
        "    images2 = preprocess_images(images2, use_multiprocessing)\r\n",
        "    mu1, sigma1 = calculate_activation_statistics(images1, batch_size)\r\n",
        "    mu2, sigma2 = calculate_activation_statistics(images2, batch_size)\r\n",
        "    fid = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\r\n",
        "    return fid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z92DzULq9LOh"
      },
      "source": [
        "def load_images(path):\r\n",
        "    \"\"\" Loads all .png or .jpg images from a given path\r\n",
        "    Warnings: Expects all images to be of same dtype and shape.\r\n",
        "    Args:\r\n",
        "        path: relative path to directory\r\n",
        "    Returns:\r\n",
        "        final_images: np.array of image dtype and shape.\r\n",
        "    \"\"\"\r\n",
        "    image_paths = []\r\n",
        "    image_extensions = [\"png\", \"jpg\"]\r\n",
        "    for ext in image_extensions:\r\n",
        "        print(\"Looking for images in\", os.path.join(path, \"*.{}\".format(ext)))\r\n",
        "        for impath in glob.glob(os.path.join(path, \"*.{}\".format(ext))):\r\n",
        "            image_paths.append(impath)\r\n",
        "    first_image = cv2.imread(image_paths[0])\r\n",
        "    W, H = first_image.shape[:2]\r\n",
        "    image_paths.sort()\r\n",
        "    image_paths = image_paths\r\n",
        "    final_images = np.zeros((len(image_paths), W, H, 3), dtype=first_image.dtype)\r\n",
        "    for idx, impath in enumerate(image_paths):\r\n",
        "        im = cv2.imread(impath)\r\n",
        "        im = im[:, :, ::-1] # Convert from BGR to RGB\r\n",
        "        assert im.dtype == final_images.dtype\r\n",
        "        final_images[idx] = im\r\n",
        "    return final_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CFn1sg59OoY",
        "outputId": "54880956-5b1b-4088-fe23-d8a232c3435a"
      },
      "source": [
        "path1 = '/content/gdrive/MyDrive/dataset/Val'\r\n",
        "path2 = '/content/gdrive/MyDrive/results_final'\r\n",
        "use_multiprocessing = False\r\n",
        "batch_size = 16\r\n",
        "\r\n",
        "images1 = load_images(path1)\r\n",
        "images2 = load_images(path2)\r\n",
        "fid_value = calculate_fid(images1, images2, use_multiprocessing, batch_size)\r\n",
        "print(fid_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking for images in /content/gdrive/MyDrive/dataset/Val/*.png\n",
            "Looking for images in /content/gdrive/MyDrive/dataset/Val/*.jpg\n",
            "Looking for images in /content/gdrive/MyDrive/results_final/*.png\n",
            "Looking for images in /content/gdrive/MyDrive/results_final/*.jpg\n",
            "257.4546569803466\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}