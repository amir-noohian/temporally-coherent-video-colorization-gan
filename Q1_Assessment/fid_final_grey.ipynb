{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fid_final_grey.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "776b5bb866a9472485ce6a49ea4463d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f5897d525a6f41148695b2cba043397f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4859b46f3e54467f91e1f9b380274034",
              "IPY_MODEL_c24ccbe2379e4acfa24f4482be3cdb24"
            ]
          }
        },
        "f5897d525a6f41148695b2cba043397f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4859b46f3e54467f91e1f9b380274034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3c589d0b360d4daa88c7d9d731dffd49",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 108857766,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 108857766,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1e67e8c1e064e78b83a6f43563a37dc"
          }
        },
        "c24ccbe2379e4acfa24f4482be3cdb24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_76e22a4a0e654e9bb217d7356286f268",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 104M/104M [00:32&lt;00:00, 3.36MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_94954eaa968f40de8592ad03d84232bf"
          }
        },
        "3c589d0b360d4daa88c7d9d731dffd49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1e67e8c1e064e78b83a6f43563a37dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "76e22a4a0e654e9bb217d7356286f268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "94954eaa968f40de8592ad03d84232bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU2wAFd5-azW",
        "outputId": "00d5a87b-f8be-4cd5-bbd4-c38bacc25228"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CInU2_rH8cGB"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torchvision.models import inception_v3\r\n",
        "import cv2\r\n",
        "import multiprocessing\r\n",
        "import numpy as np\r\n",
        "import glob\r\n",
        "import os\r\n",
        "from scipy import linalg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHcWaGyY8qeM"
      },
      "source": [
        "def to_cuda(elements):\r\n",
        "    \"\"\"\r\n",
        "    Transfers elements to cuda if GPU is available\r\n",
        "    Args:\r\n",
        "        elements: torch.tensor or torch.nn.module\r\n",
        "        --\r\n",
        "    Returns:\r\n",
        "        elements: same as input on GPU memory, if available\r\n",
        "    \"\"\"\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        return elements.cuda()\r\n",
        "    return elements"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSCRNuo68tLI"
      },
      "source": [
        "class PartialInceptionNetwork(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, transform_input=True):\r\n",
        "        super().__init__()\r\n",
        "        self.inception_network = inception_v3(pretrained=True)\r\n",
        "        self.inception_network.Mixed_7c.register_forward_hook(self.output_hook)\r\n",
        "        self.transform_input = transform_input\r\n",
        "\r\n",
        "    def output_hook(self, module, input, output):\r\n",
        "        # N x 2048 x 8 x 8\r\n",
        "        self.mixed_7c_output = output\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            x: shape (N, 3, 299, 299) dtype: torch.float32 in range 0-1\r\n",
        "        Returns:\r\n",
        "            inception activations: torch.tensor, shape: (N, 2048), dtype: torch.float32\r\n",
        "        \"\"\"\r\n",
        "        assert x.shape[1:] == (3, 299, 299), \"Expected input shape to be: (N,3,299,299)\" +\\\r\n",
        "                                             \", but got {}\".format(x.shape)\r\n",
        "        x = x * 2 -1 # Normalize to [-1, 1]\r\n",
        "\r\n",
        "        # Trigger output hook\r\n",
        "        self.inception_network(x)\r\n",
        "\r\n",
        "        # Output: N x 2048 x 1 x 1 \r\n",
        "        activations = self.mixed_7c_output\r\n",
        "        activations = torch.nn.functional.adaptive_avg_pool2d(activations, (1,1))\r\n",
        "        activations = activations.view(x.shape[0], 2048)\r\n",
        "        return activations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX5mbtfH8wqx"
      },
      "source": [
        "def get_activations(images, batch_size):\r\n",
        "    \"\"\"\r\n",
        "    Calculates activations for last pool layer for all iamges\r\n",
        "    --\r\n",
        "        Images: torch.array shape: (N, 3, 299, 299), dtype: torch.float32\r\n",
        "        batch size: batch size used for inception network\r\n",
        "    --\r\n",
        "    Returns: np array shape: (N, 2048), dtype: np.float32\r\n",
        "    \"\"\"\r\n",
        "    assert images.shape[1:] == (3, 299, 299), \"Expected input shape to be: (N,3,299,299)\" +\\\r\n",
        "                                              \", but got {}\".format(images.shape)\r\n",
        "\r\n",
        "    num_images = images.shape[0]\r\n",
        "    inception_network = PartialInceptionNetwork()\r\n",
        "    inception_network = to_cuda(inception_network)\r\n",
        "    inception_network.eval()\r\n",
        "    n_batches = int(np.ceil(num_images  / batch_size))\r\n",
        "    inception_activations = np.zeros((num_images, 2048), dtype=np.float32)\r\n",
        "    for batch_idx in range(n_batches):\r\n",
        "        start_idx = batch_size * batch_idx\r\n",
        "        end_idx = batch_size * (batch_idx + 1)\r\n",
        "\r\n",
        "        ims = images[start_idx:end_idx]\r\n",
        "        ims = to_cuda(ims)\r\n",
        "        activations = inception_network(ims)\r\n",
        "        activations = activations.detach().cpu().numpy()\r\n",
        "        assert activations.shape == (ims.shape[0], 2048), \"Expexted output shape to be: {}, but was: {}\".format((ims.shape[0], 2048), activations.shape)\r\n",
        "        inception_activations[start_idx:end_idx, :] = activations\r\n",
        "    return inception_activations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSXcLq45801q"
      },
      "source": [
        "def calculate_activation_statistics(images, batch_size):\r\n",
        "    \"\"\"Calculates the statistics used by FID\r\n",
        "    Args:\r\n",
        "        images: torch.tensor, shape: (N, 3, H, W), dtype: torch.float32 in range 0 - 1\r\n",
        "        batch_size: batch size to use to calculate inception scores\r\n",
        "    Returns:\r\n",
        "        mu:     mean over all activations from the last pool layer of the inception model\r\n",
        "        sigma:  covariance matrix over all activations from the last pool layer \r\n",
        "                of the inception model.\r\n",
        "    \"\"\"\r\n",
        "    act = get_activations(images, batch_size)\r\n",
        "    mu = np.mean(act, axis=0)\r\n",
        "    sigma = np.cov(act, rowvar=False)\r\n",
        "    return mu, sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpGmN7FW832R"
      },
      "source": [
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\r\n",
        "    \"\"\"Numpy implementation of the Frechet Distance.\r\n",
        "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\r\n",
        "    and X_2 ~ N(mu_2, C_2) is\r\n",
        "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\r\n",
        "            \r\n",
        "    Stable version by Dougal J. Sutherland.\r\n",
        "    Params:\r\n",
        "    -- mu1 : Numpy array containing the activations of the pool_3 layer of the\r\n",
        "             inception net ( like returned by the function 'get_predictions')\r\n",
        "             for generated samples.\r\n",
        "    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted\r\n",
        "               on an representive data set.\r\n",
        "    -- sigma1: The covariance matrix over activations of the pool_3 layer for\r\n",
        "               generated samples.\r\n",
        "    -- sigma2: The covariance matrix over activations of the pool_3 layer,\r\n",
        "               precalcualted on an representive data set.\r\n",
        "    Returns:\r\n",
        "    --   : The Frechet Distance.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    mu1 = np.atleast_1d(mu1)\r\n",
        "    mu2 = np.atleast_1d(mu2)\r\n",
        "\r\n",
        "    sigma1 = np.atleast_2d(sigma1)\r\n",
        "    sigma2 = np.atleast_2d(sigma2)\r\n",
        "\r\n",
        "    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\r\n",
        "    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\r\n",
        "\r\n",
        "    diff = mu1 - mu2\r\n",
        "    # product might be almost singular\r\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\r\n",
        "    if not np.isfinite(covmean).all():\r\n",
        "        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\r\n",
        "        warnings.warn(msg)\r\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\r\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\r\n",
        "\r\n",
        "    # numerical error might give slight imaginary component\r\n",
        "    if np.iscomplexobj(covmean):\r\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\r\n",
        "            m = np.max(np.abs(covmean.imag))\r\n",
        "            raise ValueError(\"Imaginary component {}\".format(m))\r\n",
        "        covmean = covmean.real\r\n",
        "\r\n",
        "    tr_covmean = np.trace(covmean)\r\n",
        "\r\n",
        "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVhA31MU9AHe"
      },
      "source": [
        "def preprocess_image(im):\r\n",
        "    \"\"\"Resizes and shifts the dynamic range of image to 0-1\r\n",
        "    Args:\r\n",
        "        im: np.array, shape: (H, W, 3), dtype: float32 between 0-1 or np.uint8\r\n",
        "    Return:\r\n",
        "        im: torch.tensor, shape: (3, 299, 299), dtype: torch.float32 between 0-1\r\n",
        "    \"\"\"\r\n",
        "    assert im.shape[2] == 3\r\n",
        "    assert len(im.shape) == 3\r\n",
        "    if im.dtype == np.uint8:\r\n",
        "        im = im.astype(np.float32) / 255\r\n",
        "    im = cv2.resize(im, (299, 299))\r\n",
        "    im = np.rollaxis(im, axis=2)\r\n",
        "    im = torch.from_numpy(im)\r\n",
        "    assert im.max() <= 1.0\r\n",
        "    assert im.min() >= 0.0\r\n",
        "    assert im.dtype == torch.float32\r\n",
        "    assert im.shape == (3, 299, 299)\r\n",
        "\r\n",
        "    return im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMtluTRM9DUw"
      },
      "source": [
        "def preprocess_images(images, use_multiprocessing):\r\n",
        "    \"\"\"Resizes and shifts the dynamic range of image to 0-1\r\n",
        "    Args:\r\n",
        "        images: np.array, shape: (N, H, W, 3), dtype: float32 between 0-1 or np.uint8\r\n",
        "        use_multiprocessing: If multiprocessing should be used to pre-process the images\r\n",
        "    Return:\r\n",
        "        final_images: torch.tensor, shape: (N, 3, 299, 299), dtype: torch.float32 between 0-1\r\n",
        "    \"\"\"\r\n",
        "    if use_multiprocessing:\r\n",
        "        with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\r\n",
        "            jobs = []\r\n",
        "            for im in images:\r\n",
        "                job = pool.apply_async(preprocess_image, (im,))\r\n",
        "                jobs.append(job)\r\n",
        "            final_images = torch.zeros(images.shape[0], 3, 299, 299)\r\n",
        "            for idx, job in enumerate(jobs):\r\n",
        "                im = job.get()\r\n",
        "                final_images[idx] = im#job.get()\r\n",
        "    else:\r\n",
        "        final_images = torch.stack([preprocess_image(im) for im in images], dim=0)\r\n",
        "    assert final_images.shape == (images.shape[0], 3, 299, 299)\r\n",
        "    assert final_images.max() <= 1.0\r\n",
        "    assert final_images.min() >= 0.0\r\n",
        "    assert final_images.dtype == torch.float32\r\n",
        "    return final_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCw1Z3_W9H4t"
      },
      "source": [
        "def calculate_fid(images1, images2, use_multiprocessing, batch_size):\r\n",
        "    \"\"\" Calculate FID between images1 and images2\r\n",
        "    Args:\r\n",
        "        images1: np.array, shape: (N, H, W, 3), dtype: np.float32 between 0-1 or np.uint8\r\n",
        "        images2: np.array, shape: (N, H, W, 3), dtype: np.float32 between 0-1 or np.uint8\r\n",
        "        use_multiprocessing: If multiprocessing should be used to pre-process the images\r\n",
        "        batch size: batch size used for inception network\r\n",
        "    Returns:\r\n",
        "        FID (scalar)\r\n",
        "    \"\"\"\r\n",
        "    images1 = preprocess_images(images1, use_multiprocessing)\r\n",
        "    images2 = preprocess_images(images2, use_multiprocessing)\r\n",
        "    mu1, sigma1 = calculate_activation_statistics(images1, batch_size)\r\n",
        "    mu2, sigma2 = calculate_activation_statistics(images2, batch_size)\r\n",
        "    fid = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\r\n",
        "    return fid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z92DzULq9LOh"
      },
      "source": [
        "def load_images(path):\r\n",
        "    \"\"\" Loads all .png or .jpg images from a given path\r\n",
        "    Warnings: Expects all images to be of same dtype and shape.\r\n",
        "    Args:\r\n",
        "        path: relative path to directory\r\n",
        "    Returns:\r\n",
        "        final_images: np.array of image dtype and shape.\r\n",
        "    \"\"\"\r\n",
        "    image_paths = []\r\n",
        "    image_extensions = [\"png\", \"jpg\"]\r\n",
        "    for ext in image_extensions:\r\n",
        "        print(\"Looking for images in\", os.path.join(path, \"*.{}\".format(ext)))\r\n",
        "        for impath in glob.glob(os.path.join(path, \"*.{}\".format(ext))):\r\n",
        "            image_paths.append(impath)\r\n",
        "    first_image = cv2.imread(image_paths[0])\r\n",
        "    W, H = first_image.shape[:2]\r\n",
        "    image_paths.sort()\r\n",
        "    image_paths = image_paths\r\n",
        "    final_images = np.zeros((len(image_paths), W, H, 3), dtype=first_image.dtype)\r\n",
        "    for idx, impath in enumerate(image_paths):\r\n",
        "        im = cv2.imread(impath)\r\n",
        "        im = im[:, :, ::-1] # Convert from BGR to RGB\r\n",
        "        assert im.dtype == final_images.dtype\r\n",
        "        final_images[idx] = im\r\n",
        "    return final_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "776b5bb866a9472485ce6a49ea4463d4",
            "f5897d525a6f41148695b2cba043397f",
            "4859b46f3e54467f91e1f9b380274034",
            "c24ccbe2379e4acfa24f4482be3cdb24",
            "3c589d0b360d4daa88c7d9d731dffd49",
            "e1e67e8c1e064e78b83a6f43563a37dc",
            "76e22a4a0e654e9bb217d7356286f268",
            "94954eaa968f40de8592ad03d84232bf"
          ]
        },
        "id": "0CFn1sg59OoY",
        "outputId": "d1fdf7ac-8686-4744-9ac0-58d94860a849"
      },
      "source": [
        "path1 = '/content/gdrive/MyDrive/dataset/Val'\r\n",
        "path2 = '/content/gdrive/MyDrive/results_final_grey'\r\n",
        "use_multiprocessing = False\r\n",
        "batch_size = 16\r\n",
        "\r\n",
        "images1 = load_images(path1)\r\n",
        "images2 = load_images(path2)\r\n",
        "fid_value = calculate_fid(images1, images2, use_multiprocessing, batch_size)\r\n",
        "print(fid_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking for images in /content/gdrive/MyDrive/dataset/Val/*.png\n",
            "Looking for images in /content/gdrive/MyDrive/dataset/Val/*.jpg\n",
            "Looking for images in /content/gdrive/MyDrive/results_final_grey/*.png\n",
            "Looking for images in /content/gdrive/MyDrive/results_final_grey/*.jpg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "776b5bb866a9472485ce6a49ea4463d4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=108857766.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "283.3576779124911\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}